{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.cloud import storage\n",
        "from urllib.parse import quote\n",
        "import re\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "68pN4BG7BjAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14991569-c044-4925-d22d-074bf280c4b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project \"ac215-decaide\""
      ],
      "metadata": {
        "id": "tEREmC-O3ART",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24d8758-3d25-4efb-bd4f-aff52414ba9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SyhFdHeH3FX",
        "outputId": "d8675bca-2b3e-4a36-8f02-97f01a2ee42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are running on a Google Compute Engine virtual machine.\n",
            "The service credentials associated with this virtual machine\n",
            "will automatically be used by Application Default\n",
            "Credentials, so it is not necessary to use this command.\n",
            "\n",
            "If you decide to proceed anyway, your user credentials may be visible\n",
            "to others with access to this virtual machine. Are you sure you want\n",
            "to authenticate with your personal account?\n",
            "\n",
            "Do you want to continue (Y/n)?  Y\n",
            "\n",
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=1leoFVBQogjT2ssjEQfM7714xajsgV&prompt=consent&token_usage=remote&access_type=offline&code_challenge=S59CkzlKtLWewqusMSnp70Tn3X2_roqISKgk37YfteU&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AVG7fiQleuMCILYH238bAFJKKaqt_yF7M8tEHc_y27f-BTGTtQ2UFtBhvuNLmMMqu22bGA\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\n",
            "Quota project \"ac215-decaide\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data\n",
        "\n",
        "GCP bucket link: https://console.cloud.google.com/storage/browser/ac215-decaide;tab=objects?forceOnBucketsSortingFiltering=true&authuser=1&project=ac215-decaide&supportedpurview=project&prefix=&forceOnObjectsSortingFiltering=false\n",
        "\n",
        "It takes 6 minutes to run the cell that loads all data from the GCP bucket subfolder"
      ],
      "metadata": {
        "id": "EZ23-luRE9ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the metadata\n",
        "# metadata_file = '/content/drive/MyDrive/APCOMP 215/AC_215/data/clean_metadata.csv'\n",
        "# metadata_df = pd.read_csv(metadata_file)\n",
        "\n",
        "# # Load images and labels\n",
        "# image_paths = []\n",
        "# images = []\n",
        "# labels = []\n",
        "\n",
        "# for filename in metadata_df['filename']:\n",
        "#     img_path = os.path.join('/content/drive/MyDrive/APCOMP 215/AC_215/data/clean_data', filename)\n",
        "#     img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
        "#     img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "#     images.append(img_array)\n",
        "#     image_paths.append(img_path)\n",
        "\n",
        "# # Convert to tensor\n",
        "# images = tf.convert_to_tensor(images, dtype=tf.float32)\n",
        "\n",
        "# # Normalize using the mean and std for ImageNet\n",
        "# mean = tf.constant([0.485, 0.456, 0.406])\n",
        "# std = tf.constant([0.229, 0.224, 0.225])\n",
        "# images = (images - mean) / std\n",
        "\n",
        "# # Encode labels\n",
        "# label_encoder = LabelEncoder()\n",
        "# encoded_labels = label_encoder.fit_transform(metadata_df['label'].values)\n",
        "# categorical_labels = tf.keras.utils.to_categorical(encoded_labels)\n",
        "\n",
        "# # Create a TensorFlow Dataset\n",
        "# dataset = tf.data.Dataset.from_tensor_slices((images, categorical_labels))\n",
        "# dataset = dataset.shuffle(buffer_size=len(images)).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "y-zyZBIgEajG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnY1BgWm_KUz"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_csv('/content/drive/MyDrive/APCOMP 215/AC_215/data/image_labels.csv')\n",
        "# df.head()\n",
        "# print(df.groupby(['label']).count())\n",
        "# plt.bar(df['label'].unique(), df.groupby(['label']).count().image)\n",
        "# plt.title('Count of Images by Year')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the metadata CSV file\n",
        "metadata_file = '/content/drive/MyDrive/APCOMP 215/AC_215/data/clean_metadata.csv'\n",
        "metadata_df = pd.read_csv(metadata_file)\n",
        "\n",
        "# Initialize the GCS client and bucket\n",
        "client = storage.Client()\n",
        "bucket_name = 'ac215-decaide'\n",
        "bucket = client.bucket(bucket_name)\n",
        "folder_path = 'images/clean_data/'\n",
        "blobs = bucket.list_blobs(prefix=folder_path) # List all blobs in the specified folder\n",
        "\n",
        "# Lists to hold images and paths\n",
        "images = []\n",
        "image_paths = []\n",
        "\n",
        "# Function to load an image from GCP bucket\n",
        "def load_image_from_gcp(bucket, image_path):\n",
        "  img_blob = bucket.blob(image_path)\n",
        "\n",
        "  # Download the image bytes and decode them\n",
        "  img_bytes = img_blob.download_as_bytes()\n",
        "  img = tf.image.decode_image(img_bytes, channels=3)\n",
        "  img = tf.image.resize(img, (224, 224))\n",
        "  return img\n",
        "\n",
        "# Load all images from the specified folder in the bucket\n",
        "for blob in blobs:\n",
        "  img_path = blob.name  # Get the full blob name (path)\n",
        "\n",
        "  try:\n",
        "    img = load_image_from_gcp(bucket, img_path)\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    images.append(img_array)\n",
        "    image_paths.append(img_path)\n",
        "  except Exception as e:\n",
        "    print(f\"Error loading {img_path}: {e}\")"
      ],
      "metadata": {
        "id": "uTRuI79kIZX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to tensor\n",
        "images = tf.convert_to_tensor(images, dtype=tf.float32)\n",
        "\n",
        "# Normalize using the mean and std for ImageNet\n",
        "mean = tf.constant([0.485, 0.456, 0.406])\n",
        "std = tf.constant([0.229, 0.224, 0.225])\n",
        "images = (images - mean) / std\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(metadata_df['label'].values)\n",
        "categorical_labels = tf.keras.utils.to_categorical(encoded_labels)\n",
        "\n",
        "# Clean and filter metadata_df since there are duplicates, and there are 5947 entries, but only 3336 images exist and were loaded from GCP\n",
        "image_paths = [path.replace('images/clean_data/', '') for path in image_paths]\n",
        "metadata_df = metadata_df.drop_duplicates(subset='filename', keep='first')\n",
        "metadata_df = metadata_df[metadata_df['filename'].isin(image_paths)]\n",
        "print(f\"Number of images in cleaned/filtered metadata_df: {metadata_df.shape[0]} \\n Number of images that exist and were loaded from GCP: {len(image_paths)}\")\n",
        "\n",
        "# Create a TensorFlow Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((images, categorical_labels))\n",
        "dataset = dataset.shuffle(buffer_size=len(images)).batch(32).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIQW8Fr_TYDr",
        "outputId": "3808cdc3-65e1-4f0c-a764-f0799e640cad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in cleaned/filtered metadata_df: 3336 \n",
            " Number of images that exist and were loaded from GCP: 3336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet50"
      ],
      "metadata": {
        "id": "clodSVkqE0vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ResNet50\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(len(label_encoder.classes_), activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze all layers except the last 10 -> fine-tune the last 10 layers\n",
        "for layer in base_model.layers[:-10]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(dataset, epochs=10)"
      ],
      "metadata": {
        "id": "RbjVSyoQEqwD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e44d23-83a9-46ee-a44e-30971c10a342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 133ms/step - accuracy: 0.1527 - loss: 31.4988\n",
            "Epoch 2/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 94ms/step - accuracy: 0.3382 - loss: 8.4209\n",
            "Epoch 3/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 94ms/step - accuracy: 0.3892 - loss: 7.1259\n",
            "Epoch 4/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 95ms/step - accuracy: 0.4200 - loss: 7.4833\n",
            "Epoch 5/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 94ms/step - accuracy: 0.4704 - loss: 7.1532\n",
            "Epoch 6/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 94ms/step - accuracy: 0.4555 - loss: 8.0584\n",
            "Epoch 7/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 92ms/step - accuracy: 0.5134 - loss: 6.2052\n",
            "Epoch 8/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 95ms/step - accuracy: 0.5107 - loss: 6.5880\n",
            "Epoch 9/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 93ms/step - accuracy: 0.5326 - loss: 6.6492\n",
            "Epoch 10/10\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 93ms/step - accuracy: 0.5344 - loss: 6.5136\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c84e0e0faf0>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Split features (images) and labels\n",
        "# X = df.drop('label', axis=1).values\n",
        "# y = df['label'].values\n",
        "\n",
        "# # Split data into train and test sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Convert labels to one-hot encoding\n",
        "# label_encoder = LabelEncoder()\n",
        "# y_train_encoded = tf.keras.utils.to_categorical(label_encoder.fit_transform(y_train))\n",
        "# y_test_encoded = tf.keras.utils.to_categorical(label_encoder.transform(y_test))\n"
      ],
      "metadata": {
        "id": "bVw5U_pLEWhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define CNN model\n",
        "# model = models.Sequential([\n",
        "#     layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224,224,3)),\n",
        "#     layers.MaxPooling2D((2, 2)),\n",
        "#     layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "#     layers.MaxPooling2D((2, 2)),\n",
        "#     layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "#     layers.Flatten(),\n",
        "#     layers.Dense(64, activation='relu'),\n",
        "#     layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
        "# ])\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='categorical_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# # Train the model\n",
        "# history = model.fit(X_train, y_train_encoded, epochs=10, validation_data=(X_test, y_test_encoded))\n",
        "\n",
        "# # Evaluate the model\n",
        "# test_loss, test_acc = model.evaluate(X_test, y_test_encoded)\n",
        "# print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "KA6mlcHeFlVF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}