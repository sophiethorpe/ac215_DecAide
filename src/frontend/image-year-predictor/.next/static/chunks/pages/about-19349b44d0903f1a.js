(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[239],{66:(e,a,o)=>{"use strict";o.r(a),o.d(a,{default:()=>r});var t=o(1106),n=o.n(t),i=o(4848);function r(){var e="#71a2c1",a="#f8f2dc",o="#5b5567",t="#f3f3eb",r={fontFamily:"Garamond, serif",color:t,fontSize:"36px",textAlign:"center",textDecoration:"none",margin:"0 30px"};return(0,i.jsxs)("div",{style:{display:"flex",flexDirection:"column",minHeight:"100vh",backgroundColor:a,color:"#333"},children:[(0,i.jsxs)("header",{style:{background:e,color:"#fff",padding:"10px 20px",display:"flex",alignItems:"center",justifyContent:"space-between"},children:[(0,i.jsx)("img",{src:"/logo_decaide-removebg-preview.png",alt:"Logo",style:{height:"150px",marginLeft:"20px",marginRight:"auto",display:"inline-block"}}),(0,i.jsxs)("nav",{style:{background:e,color:"#fff",padding:"10px 20px",display:"flex",justifyContent:"space-between",alignItems:"center",marginRight:"20px"},children:[(0,i.jsx)(n(),{href:"/",style:r,children:"Home"}),(0,i.jsx)(n(),{href:"/App",style:r,children:"App"}),(0,i.jsx)(n(),{href:"/about",style:r,children:"About"})]})]}),(0,i.jsxs)("div",{style:{background:a,textAlign:"left",padding:"10px 250px"},children:[(0,i.jsx)("h1",{style:{fontFamily:"Garamond, serif",color:o,fontSize:"54px",textAlign:"center",letterSpacing:"1px"},children:"DecAide: The Virtual Fashion Historian"}),(0,i.jsx)("p",{style:{fontFamily:"Garamond, serif",color:o,fontSize:"20px"},children:"Data Pipeline Datasets The current version of images are scraped from runway images on vougue.com. Beautifulsoup and Requests-HTML packages are used. Total of 4221 shows are scraped. Among those, we scraped ~150 shows, including all years from 1988 to 2015, resulting in ~6000 runway pictures. It takes ~3 seconds to scrape a picture. We sourced this data from here. Our dataset is downloaded and stored in a Google Cloud Bucket. Data preprocessing The processing pipeline is used to resize the images and make the resolutions compatible for our models. The output of this pipeline is resized jpg images. The processing is done locally using Docker containers, then the processed images can be uploaded to Google Cloud buckets. Data versioning We use DVC as our data versioning pipeline. Our DVC setup uses one remote. Model training and optimisation Model summary Our model employs transfer learning, making use of a ResNet50 model initially trained on the ImageNet dataset. More information on the ResNet50 Model can be found here. A global average pooling layer was added along with a dense layer and the output layer, which outputs a year category for each image. The last ten layers of the ResNet50 model were unfrozen and trained. Categorical cross-entropy was used for the loss, Adam for the optimizer, accuracy for the metric, and the model was trained for thirty epochs. Training scripts and container Experiment Tracking We use Weights and Biases to track model performance, log evaluation metrics and save the final model. Serverless Training Pipeline"})]}),(0,i.jsx)("footer",{style:{background:e,color:"#fff",padding:"10px 0",textAlign:"center",marginTop:"auto"},children:(0,i.jsx)("p",{style:{fontFamily:"Garamond, serif",color:t,fontSize:"20px"},children:"\xa9 2024 DecAide. All Rights Reserved."})})]})}},1352:(e,a,o)=>{(window.__NEXT_P=window.__NEXT_P||[]).push(["/about",function(){return o(66)}])}},e=>{var a=a=>e(e.s=a);e.O(0,[106,636,593,792],()=>a(1352)),_N_E=e.O()}]);